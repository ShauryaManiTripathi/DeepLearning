{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: /home/mtech/2021/dibyo/.anaconda/envs/GPU3_12/lib/python3.12/lib-dynload/_sqlite3.cpython-312-x86_64-linux-gnu.so: undefined symbol: sqlite3_deserialize. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import re\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_data(questions_path, misconceptions_path, train_labels_path):\n",
    "    # Load datasets\n",
    "    questions_df = pd.read_csv(questions_path)\n",
    "    misconceptions_df = pd.read_csv(misconceptions_path)\n",
    "    train_labels_df = pd.read_csv(train_labels_path)\n",
    "    \n",
    "    # Process training labels\n",
    "    train_labels = []\n",
    "    for label_str in train_labels_df['MisconceptionId']:\n",
    "        labels = [int(x) for x in label_str.split()]\n",
    "        train_labels.append(labels)\n",
    "    \n",
    "    # Create features by combining question and answer text\n",
    "    features = []\n",
    "    for idx, row in questions_df.iterrows():\n",
    "        question_text = re.sub(r'\\$.*?\\$', ' ', row['QuestionText'])  # Remove LaTeX\n",
    "        answer_texts = {\n",
    "            'A': row['AnswerAText'],\n",
    "            'B': row['AnswerBText'],\n",
    "            'C': row['AnswerCText'],\n",
    "            'D': row['AnswerDText']\n",
    "        }\n",
    "        \n",
    "        for qa_id in train_labels_df['QuestionId_Answer']:\n",
    "            q_id, ans = qa_id.split('_')\n",
    "            if int(q_id) == row['QuestionId']:\n",
    "                feature = f\"{question_text} [SEP] {answer_texts[ans]}\"\n",
    "                features.append(feature)\n",
    "    \n",
    "    return features, train_labels, len(misconceptions_df)\n",
    "\n",
    "# Model architecture\n",
    "def create_model(bert_model, num_labels, max_length=512):\n",
    "    # Input layers\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # BERT layer\n",
    "    bert_outputs = bert_model([input_ids, attention_mask])\n",
    "    sequence_output = bert_outputs[0]\n",
    "    \n",
    "    # Pooling and dense layers\n",
    "    pooled_output = GlobalAveragePooling1D()(sequence_output)\n",
    "    dropout = Dropout(0.3)(pooled_output)\n",
    "    dense1 = Dense(512, activation='relu')(dropout)\n",
    "    dropout2 = Dropout(0.2)(dense1)\n",
    "    output = Dense(num_labels, activation='sigmoid')(dropout2)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_data(texts, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask']\n",
    "    }\n",
    "\n",
    "# Training pipeline\n",
    "def train_misconception_model(features, labels, num_misconceptions):\n",
    "    # Initialize BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Tokenize data\n",
    "    tokenized_data = tokenize_data(features, tokenizer)\n",
    "    \n",
    "    # Convert labels to multi-hot encoding\n",
    "    label_matrix = np.zeros((len(labels), num_misconceptions))\n",
    "    for i, label_list in enumerate(labels):\n",
    "        label_matrix[i, label_list] = 1\n",
    "    \n",
    "    # Split data\n",
    "    train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "        tokenized_data,\n",
    "        label_matrix,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_model(bert_model, num_misconceptions)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=2e-5),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Training callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_inputs,\n",
    "        train_labels,\n",
    "        validation_data=(val_inputs, val_labels),\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, history\n",
    "\n",
    "# Prediction function\n",
    "def predict_misconceptions(model, tokenizer, texts, threshold=0.5):\n",
    "    tokenized = tokenize_data(texts, tokenizer)\n",
    "    predictions = model.predict(tokenized)\n",
    "    return (predictions > threshold).astype(int)\n",
    "\n",
    "# Generate submission file\n",
    "def create_submission(model, tokenizer, test_questions_df, output_path):\n",
    "    test_features = []\n",
    "    submission_rows = []\n",
    "    \n",
    "    for _, question in test_questions_df.iterrows():\n",
    "        question_text = re.sub(r'\\$.*?\\$', ' ', question['QuestionText'])\n",
    "        answers = {\n",
    "            'A': question['AnswerAText'],\n",
    "            'B': question['AnswerBText'],\n",
    "            'C': question['AnswerCText'],\n",
    "            'D': question['AnswerDText']\n",
    "        }\n",
    "        \n",
    "        for answer_key in answers:\n",
    "            qa_pair = f\"{question_text} [SEP] {answers[answer_key]}\"\n",
    "            test_features.append(qa_pair)\n",
    "            \n",
    "            predictions = predict_misconceptions(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                [qa_pair]\n",
    "            )[0]\n",
    "            \n",
    "            misconception_ids = ' '.join(\n",
    "                str(i+1) for i in range(len(predictions))\n",
    "                if predictions[i] == 1\n",
    "            )\n",
    "            \n",
    "            submission_rows.append({\n",
    "                'QuestionId_Answer': f\"{question['QuestionId']}_{answer_key}\",\n",
    "                'MisconceptionId': misconception_ids\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    features, labels, num_misconceptions = load_data(\n",
    "        'questions.csv',\n",
    "        'misconceptions.csv',\n",
    "        'train_labels.csv'\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, tokenizer, history = train_misconception_model(\n",
    "        features,\n",
    "        labels,\n",
    "        num_misconceptions\n",
    "    )\n",
    "    \n",
    "    # Generate predictions for test set\n",
    "    test_questions = pd.read_csv('test_questions.csv')\n",
    "    create_submission(model, tokenizer, test_questions, 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
