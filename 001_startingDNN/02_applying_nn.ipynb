{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400&display=swap');\n",
    "    .poppins-font {\n",
    "        font-family: 'Poppins', sans-serif;\n",
    "        font-size: 20px;\n",
    "    }\n",
    "</style>\n",
    "<div class=\"poppins-font\">We developed the basic understanding so far</div>\n",
    "<div class=\"poppins-font\">Now lets apply NN</div>\n",
    "\n",
    "Will i pass this subject??</br>\n",
    "input [x1,x2],</br>\n",
    "x1=no of hours of lectures attended\n",
    "x2=no of hours of project\n",
    "\n",
    "\n",
    "suppose for input 4,5 the output is 1\n",
    "but our model predicted 0.1\n",
    "\n",
    "### why its terribly wrong???\n",
    "\n",
    "because not trained, un-learnt weights, dont expect because its not trained yet\n",
    "\n",
    "we need to tell it how close its answer is to ground truth, more learning process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "<a href=\"https://ibb.co/VYJwnZm\"><img src=\"https://i.ibb.co/gZm4qpV/image.png\" alt=\"image\" border=\"0\"></a>\n",
    "\n",
    "$$\n",
    "\\text{J(w)} = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i , \\hat{y}_i)\n",
    "$$\n",
    "### this loss function is also known as empirical risk or objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary cross entropy loss function\n",
    "$$\n",
    "\\text{J(w) or BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.90335083, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "y=[0,1,1,1,1,1,0,0,0,0,0,0]\n",
    "y_pred=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.1,0.2,0.3]\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,y_pred))\n",
    "loss=tf.keras.losses.binary_crossentropy(y,y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{J(w) or MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# real valued labels\n",
    "y=[1,2,3,4,5]\n",
    "y_pred=[5,4,3,2,1]\n",
    "loss=tf.reduce_mean(tf.square(tf.subtract(y,y_pred)))\n",
    "print(loss)\n",
    "loss=tf.keras.losses.MSE(y,y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
