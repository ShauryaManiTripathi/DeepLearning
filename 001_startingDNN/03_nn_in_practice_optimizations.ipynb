{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is hard, optimization is totally different story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computationally its too hard\n",
    "<a href=\"https://ibb.co/1GZVm5s\"><img src=\"https://i.ibb.co/k2KnSf3/image.png\" alt=\"image\" border=\"0\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u start in bad place, u dead\n",
    "## To solve this\n",
    "- we can do some optimization technique to traverse the local minimas to global minima\n",
    "- make nn which has good landscape, which are more smoother\n",
    "## $n$ is the learning rate which also needs proper tuning\n",
    "- if its too small , it can get stuck in local minimas/ false local minima\n",
    "- if its too big, it overshoots the global minima\n",
    "- so we need to tune it properly, i.e. stable learning rate, ( which can overshoot local minima, but finds global minima )\n",
    "\n",
    "# billions of dimension and each dimension of W from (-inf, inf), so its not restricted and too hard to work bith , in BIG Models\n",
    "\n",
    "## soluton for learning rates\n",
    "- hit and trial\n",
    "- design a adaptive learning rate that adapts to the landscape\n",
    "\n",
    "Adaptive Learning Rates\n",
    "- Learning rates are no longer fixed\n",
    "- Can be made larger or smaller depending on:\n",
    "- how large gradient is\n",
    "- how fast learning is happening\n",
    "- size of particular weights\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent algorithms\n",
    "- Adagrad\n",
    "- RMSprop\n",
    "- Adam\n",
    "- Adadelta\n",
    "- SGD(Stochastic Gradient Descent)\n",
    "- etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.optimizers.SGD\n",
    "tf.keras.optimizers.Adam\n",
    "tf.keras.optimizers.RMSprop\n",
    "tf.keras.optimizers.Adagrad\n",
    "tf.keras.optimizers.Adadelta\n",
    "tf.keras.optimizers.Adamax\n",
    "tf.keras.optimizers.Nadam\n",
    "tf.keras.optimizers.Ftrl\n",
    "tf.keras.optimizers.Optimizer#(for class inheritance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "Kiefer & Wolfowitz. \"Stochastic Estimation of the\n",
    "Maximum of a Regression Function.\" 1952.<br>\n",
    "Kingma et al. \"Adam: A Method for Stochastic\n",
    "Optimization.\" 2014.<br>\n",
    "Zeiler et al.\"ADADELTA: An Adaptive Learning Rate\n",
    "Method.\" 2012.<br>\n",
    "Duchi et al. \"Adaptive Subgradient Methods for Online\n",
    "Learning and Stochastic Optimization.\" 20 1 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional details: http://ruderio/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
