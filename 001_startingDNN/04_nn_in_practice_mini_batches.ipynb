{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covered in 03_nn_in_practice_optimizations.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  whats not covered in previous file is covered below->>\n",
    "- about optimizatins in practice\n",
    "- about mini batches in practice\n",
    "- about overfitting in practice\n",
    "    - when the results are not generalized on unseen data\n",
    "    - underfitting when 1) model is less trained or 2) model has too less parameters(too simple)\n",
    "    - overfitting when 1) model is too complex or 2) model is trained too much\n",
    "    - how to solve overfitting\n",
    "        - get more data\n",
    "        - reduce the complexity of the model\n",
    "        - add regularization\n",
    "        - dropout\n",
    "        - data augmentation\n",
    "        - early stopping\n",
    "        - batch normalization\n",
    "        - transfer learning\n",
    "    \n",
    "- about regularization in practice\n",
    "    - technique that constrains the optimization problem to discourage complex models(avoid model from learning )\n",
    "    - improve generalization on unseen data\n",
    "\n",
    "- about dropout in practice\n",
    "- about data augmentation in practice\n",
    "- about learning rate scheduling in practice\n",
    "- about batch normalization in practice\n",
    "- about transfer learning in practice\n",
    "- about summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization type 1: Dropouts\n",
    "- Dropouts are a regularization technique where during each iteration of training, a random subset of nodes are dropped out of the neural network. This means that the weights of the dropped out nodes are not updated during that iteration. This helps in preventing overfitting.\n",
    "- During training, randomly set some of the activations to zero.\n",
    "    - Typically drop out 50% of the activations in a layer.\n",
    "    - this forces model to not rely on any one activation, and makes the model more robust.\n",
    "<a href=\"https://ibb.co/VYpnSd9\"><img src=\"https://i.ibb.co/chXfFP3/image.png\" alt=\"image\" border=\"0\"></a>\n",
    "```python\n",
    "tf.keras.layers.Dropout(rate=0.5)\n",
    "```\n",
    "- which neurons are dropped out, changes every iteration,forcing model to build different pathways , and not relying on any one activation .\n",
    "\n",
    "\n",
    "### Regularization type 2: Early stopping\n",
    "- stop training before we have chance to overfit\n",
    "</br>\n",
    "<a href=\"https://ibb.co/WNr4vxx\"><img src=\"https://i.ibb.co/TDQxMvv/Screenshot-20241009-171224.png\" alt=\"Screenshot-20241009-171224\" border=\"0\"></a><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other regularization techniques\n",
    "- L1 regularization\n",
    "- L2 regularization\n",
    "- Elastic net regularization\n",
    "- Batch normalization\n",
    "- Data augmentation\n",
    "- Transfer learning\n",
    "- Learning rate scheduling\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
